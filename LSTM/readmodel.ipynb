{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dc48d09-2025-4380-8f89-c42c7fdeac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "from model import LSTMModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Load the saved model from the file named 'model.pth'\n",
    "model_state_dict = torch.load('/pscratch/sd/h/hsko/jupyter/ML/LSTM/model_gpu/batch_run/cpuDataLoader/model_opt_adam_momentum_05_cri_rmsel1l2_eph_99900_batch_1_scheduledlr_0.01_seg_441_hidden_4_layer_1_dropout_0_logscaleFalse.pth')\n",
    "\n",
    "# Create a new instance of your model\n",
    "model = LSTMModel(1,4,10,1,0)\n",
    "\n",
    "# Load the saved parameters into the model\n",
    "# model.load_state_dict(model_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cc0e9c6-e661-4091-9f4f-e20313e88bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(1, 4, num_layers=10, batch_first=True)\n",
       "  (fc): Linear(in_features=4, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d480709-784a-4fd1-97f3-d9f424451b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING AUDIOS not in LOGSCALE\n"
     ]
    }
   ],
   "source": [
    "from dataset_cpuDataLoader import readAudio\n",
    "logscale = False\n",
    "device='cpu'\n",
    "readaudio = readAudio('/pscratch/sd/h/hsko/jupyter/jupyter/ML/FCN/sound/audio_spectrogram/dataset/DSD100/Mixtures/Dev', '/pscratch/sd/h/hsko/jupyter/jupyter/ML/FCN/sound/audio_spectrogram/dataset/DSD100/Sources/Dev', logscale, device)\n",
    "NUM_SAMPLE = 1\n",
    "audios = readaudio.__getitem__(NUM_SAMPLE, 441000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e7a2f61-e17b-4e14-9087-ce852959fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "validation_loader = DataLoader(audios, batch_size=1, shuffle=False, persistent_workers=True, num_workers=8, pin_memory=True)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "966f8926-71f2-4876-895c-afc8536cb889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bf833b5-77de-408f-870c-f4da25c91b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg val loss=0.036989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 441000, 1])) that is different to the input size (torch.Size([1, 441000, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "val_loss = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(validation_loader): \n",
    "        mixes_uncut = data[0].to(device)\n",
    "        sources_uncut = list(map(lambda x: x.to(device), data[1]))\n",
    "        mixes = mixes_uncut\n",
    "        sources = []\n",
    "        # sources = sources_uncut[:, start:end, :]\n",
    "        for tensors in sources_uncut:\n",
    "            tensor_seg = tensors\n",
    "            sources.append(tensor_seg)\n",
    "        mixes_lstm = mixes.unsqueeze(-1).float() # make dimension from (batch size, seq len) to (batch size, seq len, num of input)\n",
    "        max_mix, _ = torch.max(torch.abs(mixes_lstm), dim=1, keepdims=True)\n",
    "        mixes_lstm_norm = torch.div(mixes_lstm, max_mix)\n",
    "        mixes_lstm_norm = torch.nan_to_num(mixes_lstm_norm, nan=0.0)# switch nan to zero\n",
    "\n",
    "        # sources_lstm = torch.stack([sources[0], sources[1], sources[2], sources[3]], dim=-1).float()\n",
    "        # sources_lstm = torch.stack([sources[0], sources[1]], dim=-1).float()\n",
    "        sources_lstm = torch.stack([sources[0]], dim=-1).float()\n",
    "\n",
    "        # max_src, _ = torch.max(torch.abs(sources_lstm), dim=1, keepdims=True)\n",
    "        sources_lstm_norm = torch.div(sources_lstm, max_mix)\n",
    "        sources_lstm_norm = torch.nan_to_num(sources_lstm_norm, nan=0.0)# switch nan to zero\n",
    "\n",
    "        output = model(mixes_lstm_norm) \n",
    "        loss = criterion(output, sources_lstm_norm)\n",
    "        val_loss.append(loss.item())\n",
    "        print('Avg val loss=%f'%np.mean(val_loss))\n",
    "        bass_wav = 'bass.wav'\n",
    "        # wavfile.write(bass_wav, 44100, output[0,:,0].numpy())\n",
    "        drums_wav = 'drums.wav'\n",
    "        # wavfile.write(drums_wav, 44100, output[0,:,1].numpy())\n",
    "        wavfile.write(drums_wav, 44100, output[0,:,0].numpy())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36cfa6a-ff99-491a-8603-abd06e8e6134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-1.13.1",
   "language": "python",
   "name": "pytorch-1.13.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
